import joblib
import os
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split

# ğŸ“Œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
data_dir = "C:/Users/smhrd/scm_risk_detector_backend/raison/data"
data_path = os.path.join(data_dir, "merged_processed_data.csv")
df = pd.read_csv(data_path)

# ğŸ“Œ ì£¼ê°€ ë³€í™”ìœ¨(%) ì¶”ê°€ (OPEN ëŒ€ë¹„ HIGH, LOW, CLOSE ë³€í™”ìœ¨ ê³„ì‚°)
df['high_change'] = (df['HIGH'] - df['OPEN']) / df['OPEN']
df['low_change'] = (df['LOW'] - df['OPEN']) / df['OPEN']
df['close_change'] = (df['CLOSE'] - df['OPEN']) / df['OPEN']

# ğŸ“Œ ì…ë ¥(X)ê³¼ ì¶œë ¥(Y) ì„¤ì •
X = df[['sentiment', 'OPEN']]
Y = df[['high_change', 'low_change', 'close_change']]

# ğŸ“Œ ë°ì´í„° ë¶„í•  (í›ˆë ¨ 80%, í…ŒìŠ¤íŠ¸ 20%)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ğŸ“Œ XGBoost ëª¨ë¸ ì„¤ì •
xgb_model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=200,       # íŠ¸ë¦¬ ê°œìˆ˜ ì¦ê°€
    learning_rate=0.03,     # í•™ìŠµë¥  ê°ì†Œ (ë” ì„¸ë°€í•˜ê²Œ í•™ìŠµ)
    max_depth=6,            # íŠ¸ë¦¬ ê¹Šì´ ì¦ê°€ (ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ)
    subsample=0.8,          # ìƒ˜í”Œë§ ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€)
    colsample_bytree=0.8,   # í”¼ì²˜ ìƒ˜í”Œë§ ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€)
    min_child_weight=5,     # ìµœì†Œ ê°€ì¤‘ì¹˜ (ë¶ˆí•„ìš”í•œ ë¶„í•  ë°©ì§€)
    gamma=0.1,              # ë¦¬í”„ ë…¸ë“œ ì¶”ê°€ ê·œì œ
    early_stopping_rounds=10
)

# ğŸ“Œ ëª¨ë¸ í•™ìŠµ (ì¡°ê¸° ì¢…ë£Œ ì ìš©)
xgb_model.fit(X_train, Y_train, eval_set=[(X_test, Y_test)], verbose=True)

# ğŸ“Œ `joblib`ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì €ì¥
model_path = os.path.join(data_dir, "xgboost_stock_model.pkl")
joblib.dump(xgb_model, model_path)

print(f"âœ… XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ë¨! (íŒŒì¼: {model_path})")