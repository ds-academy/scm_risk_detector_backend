import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from sqlalchemy import create_engine
from module.data.database.db_config import db_config

# âœ… DB ì—°ê²° ì„¤ì •
engine = create_engine(f"mysql+pymysql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['database']}")

# âœ… ë‰´ìŠ¤ & ì£¼ê°€ ë°ì´í„° í´ë” ê²½ë¡œ
news_base_path = "C:/Users/smhrd/scm_risk_detector_backend/data/news"
output_path = "C:/Users/smhrd/scm_risk_detector_backend/raison/data"  # CSV ì €ìž¥ í´ë”

# âœ… news í´ë”ì—ì„œ íšŒì‚¬ ì½”ë“œ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
company_codes = [
    folder for folder in os.listdir(news_base_path)
    if os.path.isdir(os.path.join(news_base_path, folder)) and not folder.startswith("__")
]

# âœ… ëª¨ë“  íšŒì‚¬ ì½”ë“œì— ëŒ€í•´ ë°˜ë³µ ì‹¤í–‰
for company in company_codes:

    try:
        # ðŸ”¹ ì£¼ì‹ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (SQL Query ì‹¤í–‰)
        query = f"""
        SELECT DATE, OPEN, HIGH, LOW, CLOSE, VOLUME
        FROM STOCK_PRICE
        WHERE COMPANY_CODE = '{company}' AND DATE >= '2025-01-01' AND DATE <= '2025-02-03'
        """
        stock_df = pd.read_sql(query, engine)
        stock_df['DATE'] = pd.to_datetime(stock_df['DATE']).dt.date  # ë‚ ì§œ ë³€í™˜

        # ðŸ”¹ ë‰´ìŠ¤ ê°ì„± ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ê° íšŒì‚¬ë³„ `report.json` ì½ê¸°)
        news_path = os.path.join(news_base_path, company, "report.json")
        with open(news_path, 'r', encoding='utf-8') as f:
            news_data = json.load(f)
        df = pd.DataFrame.from_dict(news_data, orient='index')

        df = df[['pubDate', 'sentiment']]  # ë‚ ì§œ + ê°ì„± ì ìˆ˜ë§Œ
        df['pubDate'] = pd.to_datetime(df['pubDate']).dt.date  # ë‚ ì§œ ë³€í™˜
        df['sentiment'] = pd.to_numeric(df['sentiment'], errors='coerce')
        df = df.groupby('pubDate', as_index=False)['sentiment'].mean()  # ì¤‘ë³µ sentimentê°’ í‰ê· ì²˜ë¦¬

        # ðŸ”¹ ë°ì´í„° ë³‘í•© (`ì£¼ì‹ ë°ì´í„° ê¸°ì¤€`ìœ¼ë¡œ ë³‘í•©)
        merged_data = pd.merge(df, stock_df, left_on='pubDate', right_on='DATE', how='outer')

        # ðŸ”¹ sentiment ê°’ì„ ê°€ìž¥ ê°€ê¹Œìš´ ë‚ ì˜ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        merged_data.loc[:, 'DATE':'VOLUME'] = merged_data.loc[:, 'DATE':'VOLUME'].bfill()

        # ðŸ”¹ ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì‚­ì œ ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        merged_data.drop(columns=['DATE', 'VOLUME'], inplace=True)
        merged_data.dropna(inplace=True)
        merged_data.reset_index(drop=True, inplace=True)

        # ðŸ”¹ CSV íŒŒì¼ë¡œ ì €ìž¥
        output_file = os.path.join(output_path, f"{company}_processed.csv")
        merged_data.to_csv(output_file, index=False, encoding='utf-8-sig')

        print(f"âœ… {company} ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ & ì €ìž¥ë¨: {output_file}")
        print(merged_data)
    except Exception as e:
        print(f"âŒ {company} ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

print("ðŸŽ‰ ëª¨ë“  íšŒì‚¬ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!")

# ðŸ“Œ ë°ì´í„° ê²½ë¡œ ì„¤ì •
data_dir = "C:/Users/smhrd/scm_risk_detector_backend/raison/data"

# ðŸ“Œ `_processed.csv` íŒŒì¼ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°
csv_files = [file for file in os.listdir(data_dir) if file.endswith("_processed.csv")]

# ðŸ“Œ ë³‘í•©í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸
data_frames = []

# ðŸ“Œ íŒŒì¼ í•˜ë‚˜ì”© ë¶ˆëŸ¬ì™€ì„œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
for file in csv_files:
    file_path = os.path.join(data_dir, file)
    df = pd.read_csv(file_path)
    data_frames.append(df)

# ðŸ“Œ ëª¨ë“  ë°ì´í„°í”„ë ˆìž„ ë³‘í•©
merged_df = pd.concat(data_frames, ignore_index=True)

# ðŸ“Œ ë³‘í•©ëœ ë°ì´í„° í™•ì¸
print(merged_df)

# ðŸ“Œ CSVë¡œ ì €ìž¥ (í–¥í›„ ìž¬ì‚¬ìš© ê°€ëŠ¥)
merged_df.to_csv(os.path.join(data_dir, "merged_processed_data.csv"), index=False)
print("âœ… ëª¨ë“  CSV íŒŒì¼ì´ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤!")
